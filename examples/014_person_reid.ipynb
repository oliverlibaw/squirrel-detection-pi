{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98d9b7ce",
   "metadata": {},
   "source": [
    "# Person Re-identification using PySDK\n",
    "This notebook demonstrates Person Re-Identification (Re-ID) using PySDK. Re-ID focuses on recognizing and matching people across different camera views based on their unique appearance, like clothing and body shape.\n",
    "\n",
    "The basic pipeline works like this:\n",
    "1. Detect people in the image using a person detection model.\n",
    "2. Crop each detected person using the bounding box coordinates.\n",
    "3. Apply the Person Re-ID model to the cropped images to extract the embeddings which can further be used to identify and match individuals across different images or camera views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "802f0925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import degirum as dg, degirum_tools\n",
    "\n",
    "inference_host_address = \"@local\"\n",
    "zoo_url = \"degirum/hailo\"\n",
    "token = '' \n",
    "device_type = \"HAILORT/HAILO8L\"\n",
    "\n",
    "# Person detection model name \n",
    "person_det_model_name = \"yolov8n_relu6_person--640x640_quant_hailort_hailo8l_1\"\n",
    "\n",
    "# load AI model\n",
    "person_det_model = dg.load_model(\n",
    "    model_name=person_det_model_name,\n",
    "    inference_host_address=inference_host_address,\n",
    "    zoo_url=zoo_url,\n",
    "    token=token,\n",
    "    device_type=device_type\n",
    ")\n",
    "\n",
    "# Choose the Person reid model name \n",
    "person_reid_model_name = \"osnet_x1_0_person_reid--256x128_quant_hailort_hailo8l_1\"\n",
    "# person_reid_model_name = \"repvgg_a0_person_reid--256x128_quant_hailort_hailo8l_1\" \n",
    "\n",
    "# load AI model\n",
    "person_reid_model = dg.load_model(\n",
    "    model_name=person_reid_model_name,\n",
    "    inference_host_address=inference_host_address,\n",
    "    zoo_url=zoo_url,\n",
    "    token=token,\n",
    "    device_type=device_type\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0e54d3",
   "metadata": {},
   "source": [
    "#### Let's walk through a practical example of person re-identification.<br> \n",
    "\n",
    "## Example Scenario\n",
    "We use two sets of images:\n",
    "- **Set A:** Same person, different views.\n",
    "- **Set B:** Another individual, different views.\n",
    "\n",
    "The goal is to verify whether the ReID model can:\n",
    "\n",
    "- Correctly match all images in Set A or B as the same person.\n",
    "- Correctly distinguish Set B as a different person from Set A.\n",
    "\n",
    "This simulates a real-world scenario where the model must recognize individuals across different camera views and lighting conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fa9b414",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Utility function for displaying images\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdisplay_images\u001b[39m(images, titles=\u001b[33m\"\u001b[39m\u001b[33mImages\u001b[39m\u001b[33m\"\u001b[39m, figsize=(\u001b[32m15\u001b[39m, \u001b[32m5\u001b[39m)):\n\u001b[32m      6\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[33;03m    Display a list of images in a single row using Matplotlib.\u001b[39;00m\n\u001b[32m      8\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m \u001b[33;03m    - figsize (tuple): Size of the figure.\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "# Utility function for displaying images\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_images(images, titles=\"Images\", figsize=(15, 5)):\n",
    "    \"\"\"\n",
    "    Display a list of images in a single row using Matplotlib.\n",
    "    \n",
    "    Parameters:\n",
    "    - images (list): List of images (NumPy arrays) to display.\n",
    "    - titles (str or list): Either a single string for overall title, or list of titles for each image.\n",
    "    - figsize (tuple): Size of the figure.\n",
    "    \"\"\"\n",
    "    num_images = len(images)\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=figsize)\n",
    "    if num_images == 1:\n",
    "        axes = [axes]  # Make iterable for single image\n",
    "\n",
    "    for i, (ax, image) in enumerate(zip(axes, images)):\n",
    "        image_rgb = image[:, :, ::-1]  # Convert BGR to RGB\n",
    "        ax.imshow(image_rgb)\n",
    "        ax.axis('off')\n",
    "        if isinstance(titles, list) and i < len(titles):\n",
    "            ax.set_title(titles[i], fontsize=12)\n",
    "\n",
    "    if isinstance(titles, str):\n",
    "        fig.suptitle(titles, fontsize=16)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84e4c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image sources\n",
    "image_source_1 = \"../assets/person_1_multi_view.png\"\n",
    "image_source_2 = \"../assets/person_2_multi_view.png\"\n",
    "\n",
    "# Detections\n",
    "detections_1 = person_det_model(image_source_1)\n",
    "detections_2 = person_det_model(image_source_2)\n",
    "\n",
    "display_images([detections_1.image_overlay, detections_2.image_overlay], titles=[\"Person_1 Detections\", \"Person_2 Detections\"], figsize=(10, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d4dc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cropping\n",
    "\n",
    "# Crops from the image_source_1\n",
    "x1, y1, x2, y2 = map(int, detections_1.results[0][\"bbox\"])  # Convert bbox coordinates to integers\n",
    "person1_crop1 = detections_1.image[y1:y2, x1:x2]  # Crop the person from the image\n",
    "\n",
    "x1, y1, x2, y2 = map(int, detections_1.results[1][\"bbox\"])  # Convert bbox coordinates to integers\n",
    "person1_crop2 = detections_1.image[y1:y2, x1:x2]  # Crop the person from the image\n",
    "\n",
    "# Crops from the image_source_2\n",
    "x1, y1, x2, y2 = map(int, detections_2.results[0][\"bbox\"])  # Convert bbox coordinates to integers\n",
    "person2_crop1 = detections_2.image[y1:y2, x1:x2]  # Crop the person from the image\n",
    "\n",
    "x1, y1, x2, y2 = map(int, detections_2.results[1][\"bbox\"])  # Convert bbox coordinates to integers\n",
    "person2_crop2 = detections_2.image[y1:y2, x1:x2]  # Crop the person from the image\n",
    "\n",
    "# Display person crops\n",
    "display_images([person1_crop1, person1_crop2, person2_crop1, person2_crop2], titles=[\"Person1 Crop1\",\"Person1 Crop2\",\"Person2 Crop1\", \"Person2 Crop2\"], figsize=(10, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba56d3e8",
   "metadata": {},
   "source": [
    "### Extracting embedding using a Re-Identification (ReID) model for each person crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d782035a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#Extract the embeddings for the image_source_1 that has two crops\n",
    "embedding_person1_crop1 = np.asarray(person_reid_model(person1_crop1).results[0][\"data\"]).reshape(1, -1) # shape (1,512)\n",
    "embedding_person1_crop2 = np.asarray(person_reid_model(person1_crop2).results[0][\"data\"]).reshape(1, -1) # shape (1,512)\n",
    "\n",
    "#Extract the embeddings for the image_source_2 that detected two crops\n",
    "embedding_person2_crop1 = np.asarray(person_reid_model(person2_crop1).results[0][\"data\"]).reshape(1, -1) # shape (1,512)\n",
    "embedding_person2_crop2 = np.asarray(person_reid_model(person2_crop2).results[0][\"data\"]).reshape(1, -1) # shape (1,512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8cb4b9",
   "metadata": {},
   "source": [
    "### Calculating cosine similarity between the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dbb1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Compute cosine similarity between crops of the same person on different camera views\n",
    "similarity_person1 = cosine_similarity(embedding_person1_crop1, embedding_person1_crop2)\n",
    "print(\"Cosine similarity between Person1 crops (two images of the person1 with different camera views):\",similarity_person1)\n",
    "\n",
    "similarity_person2 = cosine_similarity(embedding_person2_crop1, embedding_person2_crop2)\n",
    "print(\"Cosine similarity between Person2 crops (two images of the person2 with different camera views):\", similarity_person2, \"\\n\")\n",
    "\n",
    "\n",
    "# Compute cosine similarity between crops of person1 and person2\n",
    "similarity_p1c1_p2c1 = cosine_similarity(embedding_person1_crop1, embedding_person2_crop1)\n",
    "print(\"Person 1 Crop 1 vs Person 2 Crop 1:\", similarity_p1c1_p2c1)\n",
    "\n",
    "similarity_p1c1_p2c2 = cosine_similarity(embedding_person1_crop1, embedding_person2_crop2)\n",
    "print(\"Person 1 Crop 1 vs Person 2 Crop 2:\", similarity_p1c1_p2c2)\n",
    "\n",
    "similarity_p1c2_p2c1 = cosine_similarity(embedding_person1_crop2, embedding_person2_crop1)\n",
    "print(\"Person 1 Crop 2 vs Person 2 Crop 1:\", similarity_p1c2_p2c1)\n",
    "\n",
    "similarity_p1c2_p2c2 = cosine_similarity(embedding_person1_crop2, embedding_person2_crop2)\n",
    "print(\"Person 1 Crop 2 vs Person 2 Crop 2:\", similarity_p1c2_p2c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237de308",
   "metadata": {},
   "source": [
    "### Interpreting Cosine Similarity Results\n",
    "These results indicate that the Re-ID Osnet model effectively distinguishes between the same and different individuals:\n",
    "\n",
    "- **Intra-person similarities** (Person 1 & Person 2 across views) are consistently **high (~0.87–0.88)**.\n",
    "- **Inter-person similarities** (between different individuals) are **significantly lower (~0.44–0.55)**.\n",
    "\n",
    "In person re-identification, it's common to apply a **similarity threshold** (typically in the range of ~0.7–0.8) to determine whether two embeddings represent the same individual. <br>\n",
    "This threshold serves as a decision boundary: if the cosine similarity between two feature vectors (embeddings) exceeds the threshold, they are considered to belong to the same person; otherwise, they are treated as different individuals.\n",
    "\n",
    "The ideal threshold may vary — it's typically fine-tuned on a validation set to achieve the best trade-off between false positives and false negatives, and it often depends on the specific model architecture and characteristics of the dataset. The choice involves balancing False positives and False negatives."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
